{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules\n",
    "\n",
    "[Association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) is a unsupervised method of generating \"If this then that\" type rules based on statistical associations in transactional data.  A rule like $\\{\\mathrm{strawberries, chocolate}\\} \\Rightarrow \\{\\mathrm{ice cream}\\}$ for if somenody buys strawberries and chocolate together, they are likely to also buy hamice cream meat. It is intended to identify \"strong rules\" that is, predictive rules.\n",
    "\n",
    "\n",
    "\n",
    "Following the original definition by [Agrawal et al.](http://dl.acm.org/citation.cfm?doid=170035.170072) the problem of association rule mining is defined as:\n",
    "Let $I=\\{i_1, i_2,\\ldots,i_n\\}$ be a set of n binary attributes called items.\n",
    "\n",
    "Let $D = \\{t_1, t_2, \\ldots, t_m\\}$ be a set of transactions called the database.\n",
    "Each transaction in $D$ has a unique transaction ID and contains a subset of the items in $I$.  \n",
    "\n",
    "A rule is defined as an implication of the form:\n",
    "$X \\Rightarrow Y$ Where $X, Y \\subseteq I and X \\cap Y = \\emptyset$.  \n",
    "\n",
    "Every rule is composed by two different set of items, also known as itemsets, X an Y, where X is called antecedent or left-hand-side (LHS) and Y consequent or right-hand-side (RHS).\n",
    "To illustrate the concepts, we use a small example from the supermarket domain. The set of items is $I= \\{\\mathrm{milk, bread, butter, cheese, macaroni}\\}$ and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represent the absence of an item in a that transaction.\n",
    "An example rule for the supermarket could be $\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}$ meaning that if butter and bread are bought, customers also buy milk.\n",
    "\n",
    "An Association Rule: is an implication of the form $X \\Rightarrow  Y$, where $X, Y \\Rightarrow I$\n",
    "\n",
    "An [association](https://en.wikipedia.org/wiki/Association_(statistics)) is any relationship between two measured quantities that renders them statistically dependent. That is,  the  [conditional probabilities] of A given B and B given A are not independent.\n",
    "\n",
    "$\\mathrm{P}(A \\cap B) = \\mathrm{P}(A)\\mathrm{P}(B) \\Leftrightarrow \\mathrm{P}(B) = \\mathrm{P}(B\\mid A)$\n",
    "\n",
    "That is, two random variables are statistically in dependent if the occurrence of B does not affect the probability of A, and vice versa. Two random variables are statistically in dependent if the occurrence of B does affect the probability of A, and vice versa. \n",
    "\n",
    "The term \"association\" is closely related to the term [correlation](https://en.wikipedia.org/wiki/Correlation_and_dependence) and to the term [mutual information](https://en.wikipedia.org/wiki/Mutual_information).  \n",
    "\n",
    "![Venn diagram dependent independent events](http://nikbearbrown.com/YouTube/MachineLearning/M04/venn_diagram_dependent_independent_events.png)\n",
    "\n",
    "*Venn diagram dependent independent events*  \n",
    "\n",
    "To illustrate the concepts, we use a small example from a small grocery. The set of items is $I= \\{\\mathrm{milk, bread, butter, cheese, macaroni}\\}$ and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represent the absence of an item in a that transaction.\n",
    "\n",
    "Example grocery database with 5 transactions and 5 items\n",
    "\n",
    "$$\n",
    "\\mathbf{tb} = \\left[\\begin{array}\n",
    "{rrrrrr}\n",
    "ID & milk & bread & butter & cheese & macaroni \\\\\n",
    "1 & 1 & 1 & 0 & 0 & 0  \\\\\n",
    "2 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "3 & 0 & 0 & 1 & 1 & 1 \\\\\n",
    "4 & 1 & 1 & 1 & 0 & 0 \\\\\n",
    "5 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "An example rule for the supermarket could be $\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}$ meaning that if butter and bread are bought, customers also buy milk.\n",
    "\n",
    "Note that even small databases many rules can be generated and we need metrics to to evaluate whether a rule can be considered statistically significant. Specifically the metrics of *support, confidence, Lift and conviction* are used as described below:\n",
    "\n",
    "Given:\n",
    " a set I of all the items; \n",
    " a database D of transactions;\n",
    " minimum support s;\n",
    " minimum confidence c; \n",
    " possibly a minimum fift l;\n",
    " possibly a minimum conviction co;\n",
    "\n",
    "Find:\n",
    " all association rules $X \\Rightarrow Y$ with a minimum support s and confidence c.\n",
    "\n",
    "\n",
    "# Support, Confidence, Lift and Conviction\n",
    "\n",
    "In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.\n",
    "Let $X$ an item-set, $X \\Rightarrow Y$ an association rule and T a set of transactions of a given database.\n",
    "\n",
    "## Support  \n",
    "\n",
    "The support value of $X$ with respect to $T$ is defined as the proportion of transactions in the database which contains the item-set $X$. That is, the fraction of transactions that contain the itemset.\n",
    "\n",
    "\n",
    "The support count ($\\sigma$) is the frequency of occurrence of an itemset\n",
    "\n",
    "\n",
    "In formula: $\\mathrm{supp}(X)=\\frac{\\sigma}{|X|}$ divides the support count by the cardinality of the item-set, $X$.\n",
    "\n",
    "In the example database, the item-set$ \\{\\mathrm{milk, bread, butter}\\}$ has a support of $2/5=0.4$ since it occurs in 40% of all transactions (2 out of 5 transactions). The argument of $\\mathrm{supp}()$ is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).  \n",
    "\n",
    "## Confidence  \n",
    "\n",
    "The confidence value of a rule, $X \\Rightarrow Y$, with respect to a set of transactions $T$, is the proportion the transactions that contains $X$ which also contains $Y$. That is, it measures how often items in $Y$ appear in transactions that contain $X$:\n",
    "\n",
    "$$\\mathrm{conf}(X \\Rightarrow Y) = \\mathrm{supp}(X \\cup Y) / \\mathrm{supp}(X)$$\n",
    "\n",
    "For example, the rule $\\{\\mathrm{butter,  bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}$ has a confidence of $0.4/0.4=1.0$ in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).  Note also that 4 of 5 times milk is bought with bread.\n",
    "\n",
    "\n",
    "Note that $supp(X \\cup Y)$ means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of events and not sets of items. We can rewrite $supp(X \\cup Y)$ as the joint probability $P(E_X \\cap E_Y)$, where $E_X and E_Y$ are the events that a transaction contains itemset X or Y, respectively.  \n",
    "\n",
    "Thus confidence can be interpreted as an estimate of the conditional probability $P(E_Y | E_X)$, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.  \n",
    "\n",
    "## Lift (Sometimes used)  \n",
    "\n",
    "The lift of a rule is defined as:  \n",
    "\n",
    "$\\mathrm{lift}(X\\Rightarrow Y) = \\frac{ \\mathrm{supp}(X \\cup Y)}{ \\mathrm{supp}(X) \\times \\mathrm{supp}(Y) }$ or the ratio of the observed support to that expected if X and Y were independent.  \n",
    "\n",
    "For Example, the rule $\\{\\mathrm{milk, bread}\\} \\Rightarrow \\{\\mathrm{butter}\\}$ has a lift of $\\frac{0.4}{0.4 \\times 0.4} = 1.25$.\n",
    "\n",
    "\n",
    "## Conviction (Sometimes used)    \n",
    "\n",
    "The conviction of a rule is defined as  $\\mathrm{conv}(X\\Rightarrow Y) =\\frac{ 1 - \\mathrm{supp}(Y) }{ 1 - \\mathrm{conf}(X\\Rightarrow Y)}$.  \n",
    "\n",
    "For Example, the rule $\\{\\mathrm{milk, bread}\\} \\Rightarrow \\{\\mathrm{butter}\\}$ has a conviction of $\\frac{1 - 0.4}{1 - .5} = 1.2$ , and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule $\\{\\mathrm{milk, bread}\\} \\Rightarrow \\{\\mathrm{butter}\\}$ would be incorrect 20% more often (1.2 times as often) if the association between $X$ and $Y$ was purely random chance.\n",
    "\n",
    "# TxP matrices, sparse matrices, adjacency lists\n",
    "\n",
    "## Sparse matrices\n",
    "\n",
    "A matrix is typically stored as a two-dimensional array. Each entry in the array represents an element ai,j of the matrix and is accessed by the two indices i and j. Conventionally, i is the row index, numbered from top to bottom, and j is the column index, numbered from left to right. For an m × n matrix, the amount of memory required to store the matrix in this format is proportional to m × n (disregarding the fact that the dimensions of the matrix also need to be stored).\n",
    "\n",
    "In a transaction database with a lot of items we would expect rows to be dominated by zeros. A customer who bought 1 item in a 10,000 item database would have a single 1 and 9,999 zeros.\n",
    "\n",
    "This is ineffecient and R has different data structures can be used to substainly reduce memory \n",
    "\n",
    "Formats can be divided into two groups:\n",
    "\n",
    "Those that support efficient modification, such as DOK (Dictionary of keys), \n",
    "LIL (List of lists), or COO (Coordinate list). These are typically used to construct the matrices.\n",
    "\n",
    "Those that support efficient access and matrix operations, such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Association rules scikit-learn\n",
    "    \n",
    "scikit-learn does not have apriori (the algorithm for Association Rules), it is very simple to write on your own or use readily available python scripts on the web for the same. See [https://github.com/asaini/Apriori](https://github.com/asaini/Apriori) or [https://github.com/timothyasp/apriori-python](https://github.com/timothyasp/apriori-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori algorithm\n",
    "\n",
    "Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.\n",
    "\n",
    "\n",
    "The Apriori algorithm was proposed by Agrawal and Srikant in 1994. Apriori is designed to operate on databases containing transactions (for example, collections of items bought by customers, or details of a website frequentation). Other algorithms are designed for finding association rules in data having no transactions (Winepi and Minepi), or having no timestamps (DNA sequencing). Each transaction is seen as a set of items (an itemset). Given a threshold $C$, the Apriori algorithm identifies the item sets which are subsets of at least $C$ transactions in the database.\n",
    "\n",
    "Apriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found.\n",
    "\n",
    "Apriori uses breadth-first search and a Hash tree (persistent data structure)|Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length $k$ from item sets of length $k-1$. Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent $k$-length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.\n",
    "\n",
    "The pseudo code for the algorithm is given below for a transaction database $T$, and a support threshold of $\\epsilon$. Usual set theoretic notation is employed, though note that $T$ is a multiset. $C_k$ is the candidate set for level $k$. At each step, the algorithm is assumed to generate the candidate sets from the large item sets of the preceding level, heeding the downward closure lemma. $count[c]$ accesses a field of the data structure that represents candidate set $c$, which is initially assumed to be zero. Many details are omitted below, usually the most important part of the implementation is the data structure used for storing the candidate sets, and counting their frequencies.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\mathrm{Apriori}(T,\\epsilon)\\\\\n",
    "&\\qquad L_1 \\gets \\{ \\mathrm{large~1-item sets} \\} \\\\\n",
    "&\\qquad k \\gets 2\\\\\n",
    "&\\qquad \\mathrm{\\textbf{while}}~ L_{k-1} \\neq \\ \\emptyset \\\\\n",
    "&\\qquad \\qquad C_k \\gets \\{ a \\cup \\{b\\} \\mid a \\in L_{k-1} \\land b \\not \\in a \\} - \\{ c \\mid \\{ s \\mid s \\subseteq c \\land |s| = k-1 \\} \\nsubseteq L_{k-1} \\}\\\\\n",
    "&\\qquad \\qquad \\mathrm{\\textbf{for}~transactions}~t \\in T\\\\\n",
    "&\\qquad \\qquad\\qquad C_t \\gets \\{ c \\mid c \\in C_k \\land c \\subseteq t \\} \\\\\n",
    "&\\qquad \\qquad\\qquad \\mathrm{\\textbf{for}~candidates}~c \\in C_t\\\\\n",
    "&\\qquad \\qquad\\qquad\\qquad \\mathit{count}[c] \\gets \\mathit{count}[c]+1\\\\\n",
    "&\\qquad \\qquad L_k \\gets \\{ c \\mid c \\in C_k \\land ~ \\mathit{count}[c] \\geq \\epsilon \\}\\\\\n",
    "&\\qquad \\qquad k \\gets k+1\\\\\n",
    "&\\qquad \\mathrm{\\textbf{return}}~\\bigcup_k L_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### Examples \n",
    "\n",
    "_ Example 1 _\n",
    "\n",
    "Consider the following database, where each row is a transaction and each cell is an individual item of the transaction:\n",
    "\n",
    "| alpha | beta | epsilon |   |\n",
    "|-------|------|---------|---|\n",
    "| alpha | beta | theta   | a |\n",
    "| alpha | beta | epsilon | a |\n",
    "| alpha | beta | theta   | b |\n",
    "\n",
    "The association rules that can be determined from this database are the following: \n",
    "\n",
    "* 100% of sets with alpha also contain beta   \n",
    "* 50% of sets with alpha, beta also have epsilon  \n",
    "* 50% of sets with alpha, beta also have theta  \n",
    "\n",
    "we can also illustrate this through a variety of examples.\n",
    "\n",
    "_ Example 2 _\n",
    "\n",
    "Assume that a large supermarket tracks sales data by stock-keeping unit (SKU) for each item: each item, such as \"butter\" or \"bread\", is identified by a numerical SKU. The supermarket has a database of transactions where each transaction is a set of SKUs that were bought together.\n",
    "\n",
    "Let the database of transactions consist of following itemsets:\n",
    "\n",
    "| Itemsets  |\n",
    "|-----------|\n",
    "| {1,2,3,4} |\n",
    "| {1,2,4}   |\n",
    "| {1,2}     |\n",
    "| {2,3,4}   |\n",
    "| {2,3}     |\n",
    "| {3,4}     |\n",
    "| {2,4}     |\n",
    "\n",
    "\n",
    "We will use Apriori to determine the frequent item sets of this database. To do so, we will say that an item set is frequent if it appears in at least 3 transactions of the database: the value 3 is the support threshold.\n",
    "\n",
    "The first step of Apriori is to count up the number of occurrences, called the support, of each member item separately. By scanning the database for the first time, we obtain the following result\n",
    "\n",
    "\n",
    "| Item | Support |\n",
    "|------|---------|\n",
    "| {1}  | 3       |\n",
    "| {2}  | 6       |\n",
    "| {3}  | 4       |\n",
    "| {4}  | 5       |\n",
    "\n",
    "\n",
    "All the itemsets of size 1 have a support of at least 3, so they are all frequent.\n",
    "\n",
    "The next step is to generate a list of all pairs of the frequent items.\n",
    "\n",
    "For example, regarding the pair {1,2}: the first table of Example 2 shows items 1 and 2 appearing together in three of the itemsets; therefore, we say item {1,2} has support of three.\n",
    "\n",
    "\n",
    "| Item  | Support |\n",
    "|-------|---------|\n",
    "| {1,2} | 3       |\n",
    "| {1,3} | 1       |\n",
    "| {1,4} | 2       |\n",
    "| {2,3} | 3       |\n",
    "| {2,4} | 4       |\n",
    "| {3,4} | 3       |\n",
    "\n",
    "The pairs {1,2}, {2,3}, {2,4}, and {3,4} all meet or exceed the minimum support of 3, so they are frequent. The pairs {1,3} and {1,4} are not. Now, because {1,3} and {1,4} are not frequent, any larger set which contains {1,3} or {1,4} cannot be frequent. In this way, we can prune sets: we will now look for frequent triples in the database, but we can already exclude all the triples that contain one of these two pairs:\n",
    "\n",
    "\n",
    "| Item    | Support |\n",
    "|---------|---------|\n",
    "| {2,3,4} | 2       |\n",
    "\n",
    "in the example, there are no frequent triplets -- {2,3,4} is below the minimal threshold, and the other triplets were excluded because they were super sets of pairs that were already below the threshold.\n",
    "\n",
    "We have thus determined the frequent sets of items in the database, and illustrated how some items were not counted because one of their subsets was already known to be below the threshold.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
